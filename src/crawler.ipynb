{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "crawler",
      "provenance": [],
      "authorship_tag": "ABX9TyNcMKP/TPYGFBijdISDlCyR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anushirahatti/web-crawler/blob/master/src/crawler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWep1e70_XX3",
        "colab_type": "text"
      },
      "source": [
        "# Web Crawling with Scrapy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UH9yhBUL_Xyg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "64dae318-32fe-4d77-b861-35833d86c753"
      },
      "source": [
        "pip install scrapy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scrapy in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "Requirement already satisfied: zope.interface>=4.1.3 in /usr/local/lib/python3.6/dist-packages (from scrapy) (5.1.0)\n",
            "Requirement already satisfied: pyOpenSSL>=16.2.0 in /usr/local/lib/python3.6/dist-packages (from scrapy) (19.1.0)\n",
            "Requirement already satisfied: protego>=0.1.15 in /usr/local/lib/python3.6/dist-packages (from scrapy) (0.1.16)\n",
            "Requirement already satisfied: lxml>=3.5.0 in /usr/local/lib/python3.6/dist-packages (from scrapy) (4.2.6)\n",
            "Requirement already satisfied: PyDispatcher>=2.0.5 in /usr/local/lib/python3.6/dist-packages (from scrapy) (2.0.5)\n",
            "Requirement already satisfied: cryptography>=2.0 in /usr/local/lib/python3.6/dist-packages (from scrapy) (2.9.2)\n",
            "Requirement already satisfied: Twisted>=17.9.0 in /usr/local/lib/python3.6/dist-packages (from scrapy) (20.3.0)\n",
            "Requirement already satisfied: cssselect>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from scrapy) (1.1.0)\n",
            "Requirement already satisfied: service-identity>=16.0.0 in /usr/local/lib/python3.6/dist-packages (from scrapy) (18.1.0)\n",
            "Requirement already satisfied: w3lib>=1.17.0 in /usr/local/lib/python3.6/dist-packages (from scrapy) (1.22.0)\n",
            "Requirement already satisfied: queuelib>=1.4.2 in /usr/local/lib/python3.6/dist-packages (from scrapy) (1.5.0)\n",
            "Requirement already satisfied: parsel>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from scrapy) (1.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from zope.interface>=4.1.3->scrapy) (47.1.1)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.6/dist-packages (from pyOpenSSL>=16.2.0->scrapy) (1.12.0)\n",
            "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.0->scrapy) (1.14.0)\n",
            "Requirement already satisfied: incremental>=16.10.1 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->scrapy) (17.5.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->scrapy) (19.3.0)\n",
            "Requirement already satisfied: hyperlink>=17.1.1 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->scrapy) (19.0.0)\n",
            "Requirement already satisfied: PyHamcrest!=1.10.0,>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->scrapy) (2.0.2)\n",
            "Requirement already satisfied: Automat>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->scrapy) (20.2.0)\n",
            "Requirement already satisfied: constantly>=15.1 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->scrapy) (15.1.0)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.6/dist-packages (from service-identity>=16.0.0->scrapy) (0.2.8)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.6/dist-packages (from service-identity>=16.0.0->scrapy) (0.4.8)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.0->scrapy) (2.20)\n",
            "Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.6/dist-packages (from hyperlink>=17.1.1->Twisted>=17.9.0->scrapy) (2.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nr6Haa7d7nNO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scrapy\n",
        "import logging\n",
        "from scrapy.crawler import CrawlerProcess"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQq0N_cN76Oj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class spider1(scrapy.Spider):\n",
        "    name = 'Wikipedia'\n",
        "    start_urls = ['https://en.wikipedia.org/wiki/Data_science']\n",
        "\n",
        "    def parse(self, response):\n",
        "        pass\n",
        "\n",
        "    logging.getLogger('scrapy').setLevel(logging.WARNING)\n",
        "\n",
        "    def parse(self, response):\n",
        "        print(response.css('h1#firstHeading::text').extract())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ouc2QNUq9VZG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c1107376-08b3-43b3-ffe9-2ee3044b0423"
      },
      "source": [
        "process = CrawlerProcess({\n",
        "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
        "})\n",
        "\n",
        "process.crawl(spider1)\n",
        "process.start()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-03 02:27:44 [scrapy.utils.log] INFO: Scrapy 2.1.0 started (bot: scrapybot)\n",
            "2020-06-03 02:27:44 [scrapy.utils.log] INFO: Versions: lxml 4.2.6.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.6.9 (default, Apr 18 2020, 01:56:04) - [GCC 8.4.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020), cryptography 2.9.2, Platform Linux-4.19.104+-x86_64-with-Ubuntu-18.04-bionic\n",
            "2020-06-03 02:27:44 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
            "2020-06-03 02:27:44 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
            "2020-06-03 02:27:44 [scrapy.extensions.telnet] INFO: Telnet Password: 06eea919522373db\n",
            "2020-06-03 02:27:44 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2020-06-03 02:27:44 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2020-06-03 02:27:44 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2020-06-03 02:27:44 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2020-06-03 02:27:44 [scrapy.core.engine] INFO: Spider opened\n",
            "2020-06-03 02:27:44 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2020-06-03 02:27:44 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2020-06-03 02:27:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Data_science> (referer: None)\n",
            "2020-06-03 02:27:45 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2020-06-03 02:27:45 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 249,\n",
            " 'downloader/request_count': 1,\n",
            " 'downloader/request_method_count/GET': 1,\n",
            " 'downloader/response_bytes': 28363,\n",
            " 'downloader/response_count': 1,\n",
            " 'downloader/response_status_count/200': 1,\n",
            " 'elapsed_time_seconds': 0.223515,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2020, 6, 3, 2, 27, 45, 56403),\n",
            " 'log_count/DEBUG': 1,\n",
            " 'log_count/INFO': 10,\n",
            " 'memusage/max': 182620160,\n",
            " 'memusage/startup': 182620160,\n",
            " 'response_received_count': 1,\n",
            " 'scheduler/dequeued': 1,\n",
            " 'scheduler/dequeued/memory': 1,\n",
            " 'scheduler/enqueued': 1,\n",
            " 'scheduler/enqueued/memory': 1,\n",
            " 'start_time': datetime.datetime(2020, 6, 3, 2, 27, 44, 832888)}\n",
            "2020-06-03 02:27:45 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['Data science']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xT1yf2R_R5r",
        "colab_type": "text"
      },
      "source": [
        "# **Reference:** \n",
        "<br/>https://www.makeuseof.com/tag/build-basic-web-crawler-pull-information-website-2/\n",
        "<br/> https://www.jitsejan.com/using-scrapy-in-jupyter-notebook.html"
      ]
    }
  ]
}